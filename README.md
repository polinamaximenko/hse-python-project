# Веб-приложение для автоматического ответа на вопросы по документу

Приложение обрабатывает текстовый документ и отвечает на вопросы пользователя по его содержимому с помощью RAG-системы.

## Функции приложения:
- Излечение текста из файлов с применением OCR
- Создание векторной базы документа
- Поиск релевантных запросу чанков по векторному сходству с FAISS
- Генерация ответа LLM на основе выделенных чанков

## Содержание

- [Быстрый старт](#Быстрый-старт)
- [Обработка документа](#Обработка-документа)
- [Векторизация](#Векторизация)
- [Подключение LLM](#Подключение-LLM)
- [Интерфейс](#Интерфейс)

## Обработка документа
- Поддерживаемые форматы файлов: PDF, DOCX, DOC, TXT, JPG, JPEG, PNG, BMP, TIFF
- OCR реализован на [pytesseract](https://pypi.org/project/pytesseract/), поддерживается русский и английский язык

### Инструкция по установке Tesseract-OCR и Poppler для распознавания PDF и изображений
#### Windows
1. Загрузите установочный пакет [Tesseract v5.5.0.20241111](https://github.com/tesseract-ocr/tesseract/releases/download/5.5.0/tesseract-ocr-w64-setup-5.5.0.20241111.exe) и установите программу в директорию `C:/Program Files/Tesseract-OCR`
2. Загрузите архив [Poppler v25.12.0-0](https://github.com/oschwartz10612/poppler-windows/releases/tag/v25.12.0-0) и распакуйте его в директорию `C:/Program Files/poppler-25.12.0`

#### Linux
Установите программы с помощью команд:
```bash
  sudo apt-get update
  sudo apt-get install tesseract-ocr
  sudo apt install poppler-utils
```
#### Mac
Установите программы с помощью команд:
```bash
  brew install tesseract
  brew install poppler
```

## Векторизация
- Для создания эмбеддингов использовалась модель [ai-forever/ru-en-RoSBERTa](https://huggingface.co/ai-forever/ru-en-RoSBERTa)

### Ключевые преимущества ru-en-RoSBERTa

#### - Удобство эмбеддингов (оптимизирована по RAG)
Модель понимает разницу между вопросом и ответом благодаря специальным префиксам:
- `search_query:` — для вопросов пользователя
- `search_document:` — для индексирования документов

#### - Двуязычность (Russian + English)

#### - Высокое качество
Базируется на RoBERTa, обучена на 4M пар текстов (supervised, synthetic, unsupervised).
За прошлый месяц модель была скачана 346,248 раз.

### Семантический поиск — FAISS:
Для поиска похожих текстов используется библиотека [FAISS](https://github.com/facebookresearch/faiss) от Facebook AI.
В рамках проекта используется версия FAISS для CPU (Python 3.11).

## Подключение LLM
Была настроена интеграция с **LLM** (инструмент **Ollama**).

- **Модель:** `gpt-oss:120b-cloud` — это открытая большая языковая модель (Open‑Weight), доступная через Ollama Cloud. Она предназначена для мощного рассуждения, генерации текста и других задач, где требуется глубокое понимание семантики. :contentReference[oaicite:0]{index=0}
- **После установки Ollama:**
  1. Запустите облачную модель:
  ```bash
    ollama run gpt-oss:120b-cloud
  ```
  2. войдите в свой аккаунт
  ```bash
    ollama signin
  ```
  

Мы использовали облачную версию модели (`-cloud`), чтобы обойти ограничения по ресурсам на локальных машинах и при этом получить мощную модель с большим количеством параметров.

Подробнее об Ollama: https://ollama.com/ :contentReference[oaicite:2]{index=2}  
Справка по модели `gpt-oss:120b-cloud`: https://ollama.com/library/gpt-oss%3A120b-cloud :contentReference[oaicite:3]{index=3}

## Интерфейс
Реализован с применением Flask с четырьмя эндпоинтами:

1. GET /: отдает HTML страницу.

2. POST /upload: принимает текст или файл, присваивает тексту id и сохраняет эту информацию в словарь, возвращает id.
-> структура позволяет разводить тексты нескольких пользователей, отправляющих одновременные запросы на сервер.

3. GET /text/<id>: возвращает текст по id.
-> если текст длиннее 150 слов, выводятся только первые 150 токенов в пользу удобочитаемости веб-страницы.

4. POST /ask принимает id текста и вопрос пользователя, возвращает ответ от LLM.

### Сильные стороны и перспективы пилотного проекта:

-> **Ограничения:**
- Сейчас приложение поддерживает только последовательные вопросы к последнему загруженному документу и не хранит контекст переписки;
- Так сейчас модель для создания эмбеддингов запускается на cpu, то большие документы будут обрабатываться довольно долго. На настоящем этапе мы отдаем предпочтение качеству, комбинируя его с апробацией нового интересного подхода, а не скорости.

-> **Перспективы:**
- Обращение к разным текстам внутри одного и того же пользовательского чата (структура фронтенда разработана с учетом этой перспективы);
- Сохранение и нормализация распознаного текста в отдельный файл для скачивания;
- Загрузка сразу нескольких файлов;
- Оптимизация решения для увеличения скорости работы приложения.


## Быстрый старт:
### 1. Установка зависимостей

#### Создайте виртуальное окружение (рекомендуется)
```bash
  python -m venv venv
```

#### Активируйте виртуальное окружение
> ##### На Windows:
> venv\Scripts\activate

> ##### На Mac/Linux:
>source venv/bin/activate

#### Установите необходимые зависимости
```bash
  pip install -r requirements.txt
```
### 2. Структура проекта
#### Репозиторий должен иметь следующую структуру:

```text
your_project/
├── app.py
└── templates/
    └── index.html
...
```
### 3. Запуск приложения
```bash
  python app.py
```

# Работа приложения
![Example](https://raw.githubusercontent.com/polinamaximenko/hse-python-project/main/example.gif)

